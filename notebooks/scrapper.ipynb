{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o WebDriver do Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Obter o site\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "# Pausar o código por 6 segundos\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscando o campo de email\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "  \n",
    "# Adicionando o email\n",
    "username.send_keys(\"seuemail@mail.com.br\")  \n",
    "  \n",
    "# Buscando o campo de senha\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "# Adicionando a senha\n",
    "pword.send_keys(\"suasenha123\")        \n",
    "\n",
    "# Fazendo o login\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "\n",
    "# Fazer a janela do navegador ficar em tela cheia\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os Headers de Analistas de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca o campo de busca\n",
    "search = driver.find_element(By.CLASS_NAME, \"search-global-typeahead__input\")\n",
    "\n",
    "# Realiza o input do item de pesquisa\n",
    "search.send_keys(\"Analista de Dados\")\n",
    "\n",
    "# Simula o pressionamento da tecla Enter\n",
    "search.send_keys(Keys.ENTER)\n",
    "\n",
    "# Adiciona um tempo de espera\n",
    "time.sleep(10)\n",
    "\n",
    "# Simula o clique no botão\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search-reusables__filters-bar\"]/ul/li[5]/button').click()\n",
    "\n",
    "# Adiciona um tempo de espera\n",
    "time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia\n",
    "headers_limpos = []\n",
    "\n",
    "# Inicia um loop para percorrer 40 páginas\n",
    "for num in range(0, 31):\n",
    "\n",
    "    # Busca o código html da página\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Transformando o código html em um objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Busca os headers\n",
    "    headers = soup.find_all('div', class_='entity-result__primary-subtitle t-14 t-black t-normal')\n",
    "\n",
    "    # Extrai cada header da div\n",
    "    for header in headers:\n",
    "        headers_limpos.append(header.get_text()[1:-1])\n",
    "\n",
    "    # Faz um scroll até o final da página\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Busca o id do botão de avançar\n",
    "    id_botao = soup.find_all('button', class_='artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view')[0]['id']\n",
    "        \n",
    "    # Avança a página    \n",
    "    driver.find_element(By.ID, id_botao).click()\n",
    "    \n",
    "\n",
    "    # Adiciona um tempo de espera de 5 segundos\n",
    "    time.sleep(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os dados em um arquivo parquet\n",
    "pd.DataFrame(headers_limpos, columns=['Header']).to_parquet(\"../data/raw/headers_Analista.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os Headers de Cientista de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca o campo de busca\n",
    "search = driver.find_element(By.CLASS_NAME, \"search-global-typeahead__input\")\n",
    "\n",
    "# Apaga o texto do campo de busca\n",
    "search.clear()\n",
    "\n",
    "# Realiza o input do item de pesquisa\n",
    "search.send_keys(\"Cientista de Dados\")\n",
    "\n",
    "# Simula o pressionamento da tecla Enter\n",
    "search.send_keys(Keys.ENTER)\n",
    "\n",
    "# Adiciona um tempo de espera\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia\n",
    "headers_limpos = []\n",
    "\n",
    "# Inicia um loop para percorrer 40 páginas\n",
    "for num in range(0, 31):\n",
    "\n",
    "    # Busca o código html da página\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Transformando o código html em um objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Busca os headers\n",
    "    headers = soup.find_all('div', class_='entity-result__primary-subtitle t-14 t-black t-normal')\n",
    "\n",
    "    # Extrai cada header da div\n",
    "    for header in headers:\n",
    "        headers_limpos.append(header.get_text()[1:-1])\n",
    "\n",
    "    # Faz um scroll até o final da página\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Busca o id do botão de avançar\n",
    "    id_botao = soup.find_all('button', class_='artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view')[0]['id']\n",
    "        \n",
    "    # Avança a página    \n",
    "    driver.find_element(By.ID, id_botao).click()\n",
    "    \n",
    "    # Adiciona um tempo de espera de 5 segundos\n",
    "    time.sleep(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os dados em um arquivo parquet\n",
    "pd.DataFrame(headers_limpos, columns=['Header']).to_parquet(\"../data/raw/headers_Cientista.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando as descrições das vagas de Analista de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simula o clique na aba de início\n",
    "driver.find_element(By.XPATH, \"//*[@id='global-nav']/div/nav/ul/li[1]/a\").click()\n",
    "\n",
    "# Interrupção do código por 5 segundos\n",
    "time.sleep(5)\n",
    "\n",
    "# Busca o campo de busca\n",
    "search = driver.find_element(By.CLASS_NAME, \"search-global-typeahead__input\")\n",
    "\n",
    "# Realiza o input do item de pesquisa\n",
    "search.send_keys('\"Analista de Dados\"')\n",
    "\n",
    "# Simula o pressionamento da tecla Enter\n",
    "search.send_keys(Keys.ENTER)\n",
    "\n",
    "# Adiciona um tempo de espera\n",
    "time.sleep(10)\n",
    "\n",
    "# Simula o clique no botão\n",
    "driver.find_element(By.XPATH, \"//*[@id='search-reusables__filters-bar']/ul/li[1]/button\").click()\n",
    "\n",
    "# Adiciona um tempo de espera\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia\n",
    "descriptions = []\n",
    "\n",
    "# Iniciando o loop\n",
    "new_loop = True\n",
    "while new_loop is True:\n",
    "    \n",
    "    # Define em quais páginas, além da 1, que serão raspadas\n",
    "    for num in range(1, 4):\n",
    "        \n",
    "        # Obtem o código html incompleto da página\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Transformando o código html em um objeto BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Buscando a primeira vaga\n",
    "        first_job = soup.find('li', class_='ember-view jobs-search-results__list-item occludable-update p0 relative scaffold-layout__list-item')\n",
    "        driver.find_element(By.ID, first_job['id']).click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Fazendo o scroll para baixo\n",
    "        element = driver.find_element(By.TAG_NAME, \"html\")\n",
    "        element.send_keys(Keys.END)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Obtem o código html completo da página\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Transformando o código html em um objeto BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "        # Itera sobre cada vaga, obtendo sua descrição\n",
    "        jobs = soup.find_all('li', class_='ember-view jobs-search-results__list-item occludable-update p0 relative scaffold-layout__list-item')\n",
    "        for item in jobs:\n",
    "            driver.find_element(By.ID, item['id']).click()\n",
    "            soup_actual = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            descricao_vaga = soup_actual.find(\"div\", class_=\"jobs-box--fadein jobs-box--full-width jobs-box--with-cta-large jobs-description jobs-description--reformatted\")\n",
    "            descriptions.append(descricao_vaga.find(\"span\").get_text()[1:-1])\n",
    "            time.sleep(5)\n",
    "            \n",
    "        # Faz um scroll até o final da página\n",
    "        element = driver.find_element(By.TAG_NAME, \"html\")\n",
    "        element.send_keys(Keys.END)\n",
    "        \n",
    "        # Pausa o código por 5 segundos\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Busca o id do botão e troca de página\n",
    "        botoes = soup.find(\"ul\", class_=\"artdeco-pagination__pages artdeco-pagination__pages--number\").find_all(\"li\")\n",
    "        botao_id = botoes[num][\"id\"]\n",
    "        driver.find_element(By.ID, botao_id).click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Encerra o loop\n",
    "        pagina_atual = botoes[num].find(\"span\").get_text()\n",
    "        if pagina_atual == \"4\":\n",
    "            new_loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva as descrições em um arquivo parquet\n",
    "pd.DataFrame(descriptions, columns=[\"Descricao_da_Vaga\"]).to_parquet(\"../data/raw/descriptions_vaga_Analista.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando as descrições das vagas de Cientista de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simula o clique na aba de início\n",
    "driver.find_element(By.XPATH, \"//*[@id='global-nav']/div/nav/ul/li[1]/a\").click()\n",
    "\n",
    "# Pausa a busca por 5 segundos\n",
    "time.sleep(10)\n",
    "\n",
    "# Busca o campo de busca\n",
    "search = driver.find_element(By.CLASS_NAME, \"search-global-typeahead__input\")\n",
    "\n",
    "# Realiza o input do item de pesquisa\n",
    "search.send_keys('\"Cientista de Dados\"')\n",
    "\n",
    "# Simula o pressionamento da tecla Enter\n",
    "search.send_keys(Keys.ENTER)\n",
    "\n",
    "# Adiciona um tempo de espera\n",
    "time.sleep(10)\n",
    "\n",
    "# Simula o clique no botão\n",
    "driver.find_element(By.XPATH, \"//*[@id='search-reusables__filters-bar']/ul/li[1]/button\").click()\n",
    "\n",
    "# Adiciona um tempo de espera\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia\n",
    "descriptions = []\n",
    "\n",
    "# Iniciando o loop\n",
    "new_loop = True\n",
    "while new_loop is True:\n",
    "    \n",
    "    # Define em quais páginas, além da 1, que serão raspadas\n",
    "    for num in range(1, 4):\n",
    "        \n",
    "        # Obtem o código html incompleto da página\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Transformando o código html em um objeto BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Buscando a primeira vaga\n",
    "        first_job = soup.find('li', class_='ember-view jobs-search-results__list-item occludable-update p0 relative scaffold-layout__list-item')\n",
    "        driver.find_element(By.ID, first_job['id']).click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Fazendo o scroll para baixo\n",
    "        element = driver.find_element(By.TAG_NAME, \"html\")\n",
    "        element.send_keys(Keys.END)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Obtem o código html completo da página\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Transformando o código html em um objeto BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "        # Itera sobre cada vaga, obtendo sua descrição\n",
    "        jobs = soup.find_all('li', class_='ember-view jobs-search-results__list-item occludable-update p0 relative scaffold-layout__list-item')\n",
    "        for item in jobs:\n",
    "            driver.find_element(By.ID, item['id']).click()\n",
    "            soup_actual = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            descricao_vaga = soup_actual.find(\"div\", class_=\"jobs-box--fadein jobs-box--full-width jobs-box--with-cta-large jobs-description jobs-description--reformatted\")\n",
    "            descriptions.append(descricao_vaga.find(\"span\").get_text()[1:-1])\n",
    "            time.sleep(5)\n",
    "            \n",
    "        # Faz um scroll até o final da página\n",
    "        element = driver.find_element(By.TAG_NAME, \"html\")\n",
    "        element.send_keys(Keys.END)\n",
    "        \n",
    "        # Pausa o código por 5 segundos\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Busca o id do botão e troca de página\n",
    "        botoes = soup.find(\"ul\", class_=\"artdeco-pagination__pages artdeco-pagination__pages--number\").find_all(\"li\")\n",
    "        botao_id = botoes[num][\"id\"]\n",
    "        driver.find_element(By.ID, botao_id).click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Encerra o loop\n",
    "        pagina_atual = botoes[num].find(\"span\").get_text()\n",
    "        if pagina_atual == \"4\":\n",
    "            new_loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva as descrições em um arquivo parquet\n",
    "pd.DataFrame(descriptions, columns=[\"Descricao_da_Vaga\"]).to_parquet(\"../data/raw/descriptions_vaga_Cientista.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fecha o navegador\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
